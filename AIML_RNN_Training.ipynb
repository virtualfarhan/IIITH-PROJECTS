{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNLSMpmzkcK8WkmJxvQPmnD",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/virtualfarhan/IIITH-PROJECTS/blob/main/AIML_RNN_Training.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Recurrent Neural Networks with PyTorch:**\n",
        "\n",
        "Recurrent Neural Networks (RNNs) are a class of neural networks particularly well-suited for sequential data processing. They are widely used in natural language processing (NLP), time series analysis, and other tasks where the order of data points matters. In this overview, we'll delve into the theory behind RNNs and how to implement them using PyTorch, a popular deep learning framework.\n",
        "\n",
        "**1. Introduction to Recurrent Neural Networks (RNNs):**\n",
        "   - RNNs are designed to handle sequences of data by maintaining an internal state or memory that captures information about previous inputs. This memory allows RNNs to process sequences of arbitrary length and learn patterns in sequential data.\n",
        "   - At each time step, an RNN takes an input vector and its internal state from the previous time step, computes a new state based on these inputs, and produces an output vector.\n",
        "\n",
        "**2. Understanding the RNN Architecture:**\n",
        "   - The basic architecture of an RNN consists of a series of recurrent units (cells) arranged in a sequence. Each recurrent unit maintains a hidden state that serves as its memory.\n",
        "   - The recurrent units share parameters across time steps, allowing them to capture temporal dependencies in the data.\n",
        "\n",
        "**3. Challenges with Standard RNNs:**\n",
        "   - While RNNs are powerful, they suffer from the vanishing gradient problem, which hinders their ability to capture long-term dependencies in sequences.\n",
        "   - Additionally, standard RNNs have difficulty retaining information from earlier time steps when processing long sequences, leading to what is known as the \"short-term memory\" problem.\n",
        "\n",
        "**4. Introduction to Long Short-Term Memory (LSTM) Networks:**\n",
        "   - To address the limitations of standard RNNs, more advanced architectures such as Long Short-Term Memory (LSTM) networks have been developed.\n",
        "   - LSTMs contain additional gating mechanisms that control the flow of information, allowing them to retain information over longer sequences and mitigate the vanishing gradient problem.\n",
        "\n",
        "**5. Implementing RNNs with PyTorch:**\n",
        "   - PyTorch provides a user-friendly interface for building and training RNNs and other neural network architectures.\n",
        "   - By leveraging PyTorch's `nn.Module` class and its built-in RNN modules (`nn.RNN`, `nn.LSTM`, `nn.GRU`), users can easily define and customize their RNN architectures.\n",
        "   - PyTorch also offers efficient handling of sequence data, automatic differentiation for gradient computation, and seamless integration with GPUs for accelerated training."
      ],
      "metadata": {
        "id": "QVf4LyA_jWOi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "import numpy as np\n",
        "\n",
        "# Define the input text\n",
        "text = ['hey how are you', 'good i am fine', 'have a nice day']\n",
        "\n",
        "# Join all the sentences together and extract the unique characters from the combined sentences\n",
        "chars = set(''.join(text))\n",
        "\n",
        "# Creating a dictionary that maps characters to integers\n",
        "char2int = {char: ind for ind, char in enumerate(chars)}\n",
        "\n",
        "# Creating another dictionary that maps integers to characters\n",
        "int2char = {ind: char for char, ind in char2int.items()}\n",
        "\n",
        "# Find the length of the longest string in the text\n",
        "maxlen = len(max(text, key=len))\n",
        "\n",
        "# Padding the sequences\n",
        "input_seq = []\n",
        "target_seq = []\n",
        "\n",
        "for i in range(len(text)):\n",
        "    input_seq.append(text[i][:-1].ljust(maxlen - 1))  # Pad the input sequence\n",
        "    target_seq.append(text[i][1:].ljust(maxlen - 1))  # Pad the target sequence\n",
        "\n",
        "# Convert characters to integers using the char2int dictionary\n",
        "input_seq = [[char2int[char] for char in seq] for seq in input_seq]\n",
        "target_seq = [[char2int[char] for char in seq] for seq in target_seq]\n",
        "\n",
        "# Convert to numpy arrays\n",
        "input_seq = np.array(input_seq)\n",
        "target_seq = np.array(target_seq)\n",
        "\n",
        "# One-hot encode the input sequences\n",
        "def one_hot_encode(sequence, dict_size, seq_len):\n",
        "    features = np.zeros((sequence.shape[0], seq_len, dict_size), dtype=np.float32)\n",
        "    for i in range(len(sequence)):\n",
        "        for u in range(seq_len):\n",
        "            features[i, u, sequence[i][u]] = 1\n",
        "    return features\n",
        "\n",
        "dict_size = len(char2int)\n",
        "seq_len = maxlen - 1  # Since we removed one character for input and target sequences\n",
        "batch_size = len(text)\n",
        "\n",
        "input_seq = one_hot_encode(input_seq, dict_size, seq_len)\n",
        "print(\"Input shape: {} --> (Batch Size, Sequence Length, One-Hot Encoding Size)\".format(input_seq.shape))\n",
        "\n",
        "input_seq = torch.from_numpy(input_seq)\n",
        "target_seq = torch.Tensor(target_seq)\n",
        "\n",
        "# Check if GPU is available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "class Model(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_dim, n_layers):\n",
        "        super(Model, self).__init__()\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.n_layers = n_layers\n",
        "        self.rnn = nn.RNN(input_size, hidden_dim, n_layers, batch_first=True)\n",
        "        self.fc = nn.Linear(hidden_dim, output_size)\n",
        "\n",
        "    def forward(self, x, hidden):\n",
        "        batch_size = x.size(0)\n",
        "        out, hidden = self.rnn(x, hidden)\n",
        "        out = out.contiguous().view(-1, self.hidden_dim)\n",
        "        out = self.fc(out)\n",
        "        return out, hidden\n",
        "\n",
        "    def init_hidden(self, batch_size):\n",
        "        return torch.zeros(self.n_layers, batch_size, self.hidden_dim).to(device)\n",
        "\n",
        "# Instantiate the model\n",
        "model = Model(input_size=dict_size, output_size=dict_size, hidden_dim=10, n_layers=1)\n",
        "model = model.to(device)\n",
        "\n",
        "# Define hyperparameters\n",
        "n_epochs = 100\n",
        "lr = 0.01\n",
        "\n",
        "# Define Loss, Optimizer\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
        "\n",
        "# Training Run\n",
        "input_seq = input_seq.to(device)\n",
        "target_seq = target_seq.to(device)\n",
        "\n",
        "for epoch in range(1, n_epochs + 1):\n",
        "    optimizer.zero_grad()\n",
        "    hidden = model.init_hidden(batch_size)\n",
        "    output, hidden = model(input_seq, hidden)\n",
        "    loss = criterion(output, target_seq.view(-1).long())\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if epoch % 10 == 0:\n",
        "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
        "        print(\"Loss: {:.4f}\".format(loss.item()))\n",
        "\n",
        "def predict(model, character, hidden):\n",
        "    character = np.array([[char2int[c] for c in character]])\n",
        "    character = one_hot_encode(character, dict_size, character.shape[1])\n",
        "    character = torch.from_numpy(character).float().to(device)\n",
        "    out, hidden = model(character, hidden)\n",
        "    prob = nn.functional.softmax(out[-1], dim=0).data\n",
        "    char_ind = torch.max(prob, dim=0)[1].item()\n",
        "    return int2char[char_ind], hidden\n",
        "\n",
        "def sample(model, out_len, start='hey'):\n",
        "    model.eval()\n",
        "    start = start.lower()\n",
        "    chars = [ch for ch in start]\n",
        "    hidden = model.init_hidden(1)\n",
        "    for _ in range(out_len - len(start)):\n",
        "        char, hidden = predict(model, chars[-1], hidden)\n",
        "        chars.append(char)\n",
        "    return ''.join(chars)\n",
        "\n",
        "# Set the desired length of the generated sample\n",
        "out_len = 4\n",
        "\n",
        "generated_sample = sample(model,out_len,'good')\n",
        "print(\"Generated Sample:\", generated_sample)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-gFsiRc8cbZp",
        "outputId": "a2af9b57-1ea2-47b9-c245-af33a4c2187d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Input shape: (3, 14, 17) --> (Batch Size, Sequence Length, One-Hot Encoding Size)\n",
            "Using device: cpu\n",
            "Epoch: 10/100............. Loss: 2.4282\n",
            "Epoch: 20/100............. Loss: 2.2164\n",
            "Epoch: 30/100............. Loss: 1.9051\n",
            "Epoch: 40/100............. Loss: 1.5340\n",
            "Epoch: 50/100............. Loss: 1.1665\n",
            "Epoch: 60/100............. Loss: 0.8531\n",
            "Epoch: 70/100............. Loss: 0.6207\n",
            "Epoch: 80/100............. Loss: 0.4625\n",
            "Epoch: 90/100............. Loss: 0.3519\n",
            "Epoch: 100/100............. Loss: 0.2709\n",
            "Generated Sample: good\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Data Preparation and Preprocessing:**\n",
        "\n",
        "Before delving into model creation and training, it's crucial to prepare the data adequately. We begin by compiling a collection of motivational quotes, intending to train our model to generate similar phrases. These quotes are combined, and from this amalgamation, we derive a set of unique characters to encompass the textual diversity. By mapping each character to a corresponding integer and vice versa, we establish dictionaries for efficient encoding and decoding operations. Furthermore, we ascertain the length of the longest quote to facilitate uniform padding across sequences.\n",
        "\n",
        "1. **One-Hot Encoding and Sequence Padding**:\n",
        "   - To enable effective processing by our neural network, we encode each character using the one-hot encoding technique. This transformation converts characters into binary vectors, with each vector representing the presence or absence of a specific character in the sequence.\n",
        "   - Additionally, we pad the input and target sequences to ensure uniformity in length. By extending or truncating sequences as necessary, we guarantee consistency in the model's input dimensions.\n",
        "\n",
        "2. **Model Architecture Specification**:\n",
        "   - Our model architecture is based on a recurrent neural network (RNN), a type of neural network particularly adept at capturing sequential dependencies in data. Employing PyTorch's `nn.Module`, we define an RNN model equipped with both an RNN layer and a linear layer.\n",
        "   - The RNN layer serves as the core component responsible for processing sequential data, while the linear layer transforms the output to align with the dimensions of the one-hot encoded vectors.\n",
        "\n",
        "3. **Training and Optimization**:\n",
        "   - With the model architecture defined, we proceed to configure the training process. Key hyperparameters such as the number of epochs and the learning rate are specified to govern the training dynamics.\n",
        "   - To optimize the model's performance, we employ the cross-entropy loss function and the Adam optimizer. These components work in tandem to minimize the discrepancy between predicted and actual outputs, thereby refining the model's predictive capabilities.\n",
        "   - Throughout the training process, we iteratively update the model's parameters based on computed gradients, gradually improving its ability to generate coherent text.\n",
        "\n",
        "4. **Text Generation and Inference**:\n",
        "   - Once the model is trained, we leverage it to generate new text samples. Using a seed input as a starting point, we invoke the model's predictive capabilities to generate subsequent characters.\n",
        "   - Through this process of sampling, the model produces sequences of text that exhibit coherence and semblance to the motivational quotes it was trained on."
      ],
      "metadata": {
        "id": "Vd7aBy0wiVrt"
      }
    }
  ]
}